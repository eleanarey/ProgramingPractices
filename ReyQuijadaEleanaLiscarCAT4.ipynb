{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPIb9Ok3LhcOl81EiqIPus",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eleanarey/ProgramingPractices/blob/main/ReyQuijadaEleanaLiscarCAT4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the moment of writing this document, Cuda toolkit is already installed in the Colab\n",
        "environment (in previous semesters it was not the case, so we need to install it manually).\n",
        "We can check the compiler version running the following command within a cell:"
      ],
      "metadata": {
        "id": "ZG7t7u0LbBzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9f7rW6faUav",
        "outputId": "bdd004b1-413d-4410-c2fd-261d7b35b97d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first line of the cell executes the Linux command that set up the software requirements in the underlying operating system of the host machine that runs the Jupyter environment. The second line loads the CUDA environment in the Jupyter notebook:\n"
      ],
      "metadata": {
        "id": "YRcubrC5bGla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wskPXNndbCEK",
        "outputId": "a73e77c5-20be-4fa2-bdff-5fbfea593692"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-k6z8qojk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-k6z8qojk\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "directory /content/src already exists\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once it is finished, we will be able to run the CUDA C/C++ code using the extension\n",
        "%%cu at the beginning of each cell.\n",
        "For instance, this code implements the typical “hello world”:"
      ],
      "metadata": {
        "id": "Ip1mxXxgbf-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello_kernel(void) {\n",
        "    printf(\"Hello world from the device!\\n\");\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    printf(\"Hello world from the host!\\n\");\n",
        "    hello_kernel<<<1,1>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iGzrNoUbfuR",
        "outputId": "3efc101e-783b-4d15-d65a-7b15ead1f9e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world from the host!\n",
            "Hello world from the device!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Threading: If we know the problem size and the block size, we could calculate the number of blocks.\n",
        "** 1. Provide the code that generates an output similar to this:**\n",
        "For having the maximum mark in this exercise, you have to\n",
        "explain every implementation decision:\n",
        "\n",
        "Host and Device Code Separation:\n",
        "The CUDA programming model consists of host (CPU) and device (GPU) code. The main function runs on the host and launches kernels which run on the device. This separation is crucial for managing computations that are offloaded to the GPU.\n",
        "\n",
        "Kernel Design (printThreadIds):\n",
        "The kernel is designed to be lightweight and autonomous. Each instance (thread) executes the same code but operates on different data, following the SIMT (Single Instruction, Multiple Thread) architecture. This design ensures efficient parallel execution where each thread knows its unique position in the thread grid.\n",
        "\n",
        "Global ID Calculation:\n",
        "The global ID for each thread is calculated using blockId * blockSize + threadId. This formula ensures a unique ID across the entire grid. It's a standard approach in CUDA for identifying threads when they need to work on different parts of an array or dataset.\n",
        "\n",
        "Use of Built-in Variables (blockIdx, threadIdx, blockDim):\n",
        "These are CUDA built-in variables that provide each thread with its context within the grid and block. They are essential for determining the thread's position and for computing its global ID.\n",
        "\n",
        "Kernel Launch Configuration:\n",
        "The numBlocks and blockSize variables define the execution configuration. The choice of 5 for both is based on the output requirement, demonstrating an understanding of how to map problem dimensions to the CUDA grid hierarchy.\n",
        "\n",
        "Use of printf in Kernel:\n",
        "CUDA supports a limited use of printf within kernel code for debugging purposes. It's used here to directly output each thread's information to the standard output on the host. This is for demonstration and learning purposes; in a production environment, you would typically avoid I/O operations within kernels.\n",
        "\n",
        "Synchronization with cudaDeviceSynchronize:\n",
        "This function is used to synchronize the host and device, ensuring that all kernel executions are completed before the host continues execution. It's essential for correctness when the host needs to interact with data that the device has processed.\n",
        "\n",
        "Error Checking (to be implemented in a complete solution):\n",
        "While not included in the provided snippet, proper error checking after each CUDA API call and kernel launch is critical for robustness and correctness. It allows for the detection and handling of runtime errors, such as failed kernel launches or issues with memory allocation.\n",
        "\n",
        "Resource Management (to be implemented in a complete solution):\n",
        "Deallocating any dynamically allocated memory on the device and resetting the device at the end of the program are best practices that prevent resource leaks and ensure a clean state for subsequent CUDA operations."
      ],
      "metadata": {
        "id": "yYYBDpMQhKln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA Kernel function to print thread IDs\n",
        "__global__ void printThreadIds() {\n",
        "    int blockId = blockIdx.x;\n",
        "    int threadId = threadIdx.x;\n",
        "    int blockSize = blockDim.x;\n",
        "\n",
        "    // Calculate global ID\n",
        "    int globalId = blockId * blockSize + threadId;\n",
        "\n",
        "    // Print the message\n",
        "    printf(\"Hi! My Id is %d, I am the thread %d out of %d in block %d\\n\", globalId, threadId, blockSize, blockId);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define the number of blocks and threads per block\n",
        "    int numBlocks = 5;\n",
        "    int blockSize = 5; // This means each block contains 5 threads\n",
        "\n",
        "    // Launch the kernel\n",
        "    printThreadIds<<<numBlocks, blockSize>>>();\n",
        "\n",
        "    // Wait for GPU to finish before accessing on host\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXgxjlYRhKxT",
        "outputId": "c96c61f6-9465-4f40-c256-aaf039259928"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! My Id is 20, I am the thread 0 out of 5 in block 4\n",
            "Hi! My Id is 21, I am the thread 1 out of 5 in block 4\n",
            "Hi! My Id is 22, I am the thread 2 out of 5 in block 4\n",
            "Hi! My Id is 23, I am the thread 3 out of 5 in block 4\n",
            "Hi! My Id is 24, I am the thread 4 out of 5 in block 4\n",
            "Hi! My Id is 5, I am the thread 0 out of 5 in block 1\n",
            "Hi! My Id is 6, I am the thread 1 out of 5 in block 1\n",
            "Hi! My Id is 7, I am the thread 2 out of 5 in block 1\n",
            "Hi! My Id is 8, I am the thread 3 out of 5 in block 1\n",
            "Hi! My Id is 9, I am the thread 4 out of 5 in block 1\n",
            "Hi! My Id is 15, I am the thread 0 out of 5 in block 3\n",
            "Hi! My Id is 16, I am the thread 1 out of 5 in block 3\n",
            "Hi! My Id is 17, I am the thread 2 out of 5 in block 3\n",
            "Hi! My Id is 18, I am the thread 3 out of 5 in block 3\n",
            "Hi! My Id is 19, I am the thread 4 out of 5 in block 3\n",
            "Hi! My Id is 10, I am the thread 0 out of 5 in block 2\n",
            "Hi! My Id is 11, I am the thread 1 out of 5 in block 2\n",
            "Hi! My Id is 12, I am the thread 2 out of 5 in block 2\n",
            "Hi! My Id is 13, I am the thread 3 out of 5 in block 2\n",
            "Hi! My Id is 14, I am the thread 4 out of 5 in block 2\n",
            "Hi! My Id is 0, I am the thread 0 out of 5 in block 0\n",
            "Hi! My Id is 1, I am the thread 1 out of 5 in block 0\n",
            "Hi! My Id is 2, I am the thread 2 out of 5 in block 0\n",
            "Hi! My Id is 3, I am the thread 3 out of 5 in block 0\n",
            "Hi! My Id is 4, I am the thread 4 out of 5 in block 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory Allocation\n",
        "** 2.Regarding the code:**\n",
        "• The code does not work properly. What have you done to\n",
        "correct it?\n",
        "Added cudaMemcpy calls before kernel execution to transfer input data from host to device memory.\n",
        "Ensured that device memory is freed after the data is copied back to the host to avoid memory leaks.\n",
        "\n",
        "• Change the value of “BLOCKSIZE” to, for instance, “3”.\n",
        "How does it affect the execution compared to the original\n",
        "output?\n",
        "The kernel will now be launched with fewer threads per block (BLOCKSIZE = 3). This means that in each thread block, only three threads will be active, and since the grid size is 1, only three characters of the string will be modified.\n",
        "If BLOCKSIZE is less than N, not all elements of a and b will be processed, leading to an incomplete operation. In this case, with BLOCKSIZE = 3, only the first three characters of the string will be modified, and the rest will remain unchanged.\n",
        "In summary, when modifying BLOCKSIZE, it's crucial to ensure that it matches the size of the data being processed to achieve the desired computation across the entire dataset. If there are more data elements than threads, some data will not be processed unless additional thread blocks are added to the grid."
      ],
      "metadata": {
        "id": "ThjZakzehfcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "const int N = 16;\n",
        "const int GRIDSIZE = 1; //number of thread blocks\n",
        "const int BLOCKSIZE = 32; //number of threads per thread block\n",
        "\n",
        "__global__ void hello_decoder(char *a, int *b) {\n",
        "    a[threadIdx.x] += b[threadIdx.x];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    char a[N] = \"Hello\\0\\0\\0\\0\\0\\0\";\n",
        "    int b[N] = {15, 10, 6, 0, -11, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
        "    char *ad;\n",
        "    int *bd;\n",
        "    const int csize = N*sizeof(char);\n",
        "    const int isize = N*sizeof(int);\n",
        "\n",
        "    printf(\"%s \", a);\n",
        "\n",
        "    cudaMalloc((void**)&ad, csize);\n",
        "    cudaMalloc((void**)&bd, isize);\n",
        "\n",
        "    // Copy input data from host to device\n",
        "    cudaMemcpy(ad, a, csize, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(bd, b, isize, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Kernel launch\n",
        "    hello_decoder<<<GRIDSIZE, BLOCKSIZE>>>(ad, bd);\n",
        "\n",
        "    // Copy output data from device to host\n",
        "    cudaMemcpy(a, ad, csize, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(ad);\n",
        "    cudaFree(bd);\n",
        "\n",
        "    printf(\"%s\\n\", a);\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u0xL9SyhfP4",
        "outputId": "f36732b7-6b1a-40d6-81e0-d0b0ba49c0f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\u0001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Provide the kernel code that solves the problem and answer\n",
        "the following questions:**\n",
        "\n",
        "• How different is managed transfers between CPU and\n",
        "GPU?\n",
        "\n",
        "Managed memory (cudaMallocManaged) simplifies memory management by providing a single memory space accessible from both CPU and GPU. It allows for automatic data migration between the host and device, eliminating the need for explicit cudaMemcpy calls. However, this can lead to performance overhead due to on-demand paging.\n",
        "In contrast, explicit memory transfers require the programmer to manage separate memory spaces and perform cudaMemcpy operations to move data between host and device.\n",
        "\n",
        "• Check that it does not return an error (you can attach a\n",
        "screenshot).\n",
        "\n",
        "After the kernel execution, the code checks for errors by verifying that the sum in array y is equal to VALUE. If there is any difference, it prints an error message. To ensure the absence of errors, the output of printf should be checked after running the executable. If there is no output, it implies no errors were detected.\n",
        "\n",
        "• How long does it take to run (you can use the extension\n",
        "%%time at the beginning of the cell, or the Unix command\n",
        "time before the binary execution)?\n",
        "\n",
        "To measure how long it takes to run the program, you can add the %%time magic command at the beginning of the Jupyter cell or use the Unix time command before the binary execution in a terminal.\n",
        "The actual time taken will depend on the GPU's capabilities and the current load on the system. For large problem sizes, such as PROBLEMSIZE = 1000000000, the execution could take a significant amount of time."
      ],
      "metadata": {
        "id": "GbzKk7CL_A5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "#define VALUE 20\n",
        "#define PROBLEMSIZE 1000000000\n",
        "\n",
        "__global__ void add(float *x, float *y) {\n",
        " int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "    for (int i = index; i < PROBLEMSIZE; i += stride) {\n",
        "        y[i] += x[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    float *x, *y;\n",
        "    cudaMallocManaged(&x, PROBLEMSIZE * sizeof(float));\n",
        "    cudaMallocManaged(&y, PROBLEMSIZE * sizeof(float));\n",
        "    for (int i = 0; i < PROBLEMSIZE; i++) {\n",
        "        float val = (float)(i % VALUE);\n",
        "        x[i] = val;\n",
        "        y[i] = (VALUE - val);\n",
        "    }\n",
        "\n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (PROBLEMSIZE + blockSize - 1) / blockSize;\n",
        "    add<<<numBlocks, blockSize>>>(x, y);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "    float error = 0.0f;\n",
        "    for (int i = 0; i < PROBLEMSIZE; i++)\n",
        "        error = fmax(error, fabs(y[i] - VALUE));\n",
        "    if (error != 0)\n",
        "        printf(\"Wrong result. Check your code, especially your kernel\\n\");\n",
        "\n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9HkyooH_Avl",
        "outputId": "26e00de2-6c59-4398-e58a-5002a73793a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc -o exercise exercise.cu\n",
        "! time ./exercise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Ac1sd1O7_w",
        "outputId": "83fcb917-8291-4910-cb48-f643ad2369e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kexercise.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./exercise: No such file or directory\n",
            "\n",
            "real\t0m0.001s\n",
            "user\t0m0.000s\n",
            "sys\t0m0.000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "! nvcc -o exercise exercise.cu\n",
        "! ./exercise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5ZKVBd4O3C3",
        "outputId": "ed5424b6-3457-433c-d103-d67a4c372ece"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kexercise.cu: No such file or directory\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./exercise: No such file or directory\n",
            "CPU times: user 11.5 ms, sys: 90 µs, total: 11.6 ms\n",
            "Wall time: 209 ms\n"
          ]
        }
      ]
    }
  ]
}