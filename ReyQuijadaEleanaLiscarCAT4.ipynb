{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQDqAee2rpvYzrMZzkAU/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eleanarey/ProgramingPractices/blob/main/ReyQuijadaEleanaLiscarCAT4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the moment of writing this document, Cuda toolkit is already installed in the Colab\n",
        "environment (in previous semesters it was not the case, so we need to install it manually).\n",
        "We can check the compiler version running the following command within a cell:"
      ],
      "metadata": {
        "id": "ZG7t7u0LbBzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9f7rW6faUav",
        "outputId": "e3193b7d-4ab5-4501-aa0f-7537c83aa3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first line of the cell executes the Linux command that set up the software requirements in the underlying operating system of the host machine that runs the Jupyter environment. The second line loads the CUDA environment in the Jupyter notebook:\n"
      ],
      "metadata": {
        "id": "YRcubrC5bGla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wskPXNndbCEK",
        "outputId": "20f940de-2c17-4f9b-b1b0-52c7cfa86e7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-5_vesuil\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-5_vesuil\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0d2ab99cccbbc682722e708515fe9c4cfc50185a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4716 sha256=a970e43e8b3282005b92a3e656462936fde0268048f05c898c8887e184fa722d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-aspfjpu4/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once it is finished, we will be able to run the CUDA C/C++ code using the extension\n",
        "%%cu at the beginning of each cell.\n",
        "For instance, this code implements the typical “hello world”:"
      ],
      "metadata": {
        "id": "Ip1mxXxgbf-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello_kernel(void) {\n",
        "    printf(\"Hello world from the device!\\n\");\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    printf(\"Hello world from the host!\\n\");\n",
        "    hello_kernel<<<1,1>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iGzrNoUbfuR",
        "outputId": "ca992cb8-e730-4e5b-fa25-105f1de0d579"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world from the host!\n",
            "Hello world from the device!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Threading: If we know the problem size and the block size, we could calculate the number of blocks.\n",
        "\n",
        "### 1. Provide the code that generates an output similar to this: For having the maximum mark in this exercise, you have to explain every implementation decision:\n",
        "\n",
        "- Host and Device Code Separation:\n",
        "The CUDA programming model consists of host (CPU) and device (GPU) code. The main function runs on the host and launches kernels which run on the device. This separation is crucial for managing computations that are offloaded to the GPU.\n",
        "\n",
        "- Kernel Design (printThreadIds):\n",
        "The kernel is designed to be lightweight and autonomous. Each instance (thread) executes the same code but operates on different data, following the SIMT (Single Instruction, Multiple Thread) architecture. This design ensures efficient parallel execution where each thread knows its unique position in the thread grid.\n",
        "\n",
        "- Global ID Calculation:\n",
        "The global ID for each thread is calculated using blockId * blockSize + threadId. This formula ensures a unique ID across the entire grid. It's a standard approach in CUDA for identifying threads when they need to work on different parts of an array or dataset.\n",
        "\n",
        "- Use of Built-in Variables (blockIdx, threadIdx, blockDim):\n",
        "These are CUDA built-in variables that provide each thread with its context within the grid and block. They are essential for determining the thread's position and for computing its global ID.\n",
        "\n",
        "- Kernel Launch Configuration:\n",
        "The numBlocks and blockSize variables define the execution configuration. The choice of 5 for both is based on the output requirement, demonstrating an understanding of how to map problem dimensions to the CUDA grid hierarchy.\n",
        "\n",
        "- Use of printf in Kernel:\n",
        "CUDA supports a limited use of printf within kernel code for debugging purposes. It's used here to directly output each thread's information to the standard output on the host. This is for demonstration and learning purposes; in a production environment, you would typically avoid I/O operations within kernels.\n",
        "\n",
        "- Synchronization with cudaDeviceSynchronize:\n",
        "This function is used to synchronize the host and device, ensuring that all kernel executions are completed before the host continues execution. It's essential for correctness when the host needs to interact with data that the device has processed.\n",
        "\n",
        "- Error Checking (to be implemented in a complete solution):\n",
        "While not included in the provided snippet, proper error checking after each CUDA API call and kernel launch is critical for robustness and correctness. It allows for the detection and handling of runtime errors, such as failed kernel launches or issues with memory allocation.\n",
        "\n",
        "- Resource Management (to be implemented in a complete solution):\n",
        "Deallocating any dynamically allocated memory on the device and resetting the device at the end of the program are best practices that prevent resource leaks and ensure a clean state for subsequent CUDA operations."
      ],
      "metadata": {
        "id": "yYYBDpMQhKln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA Kernel function to print thread IDs\n",
        "__global__ void printThreadIds() {\n",
        "    int blockId = blockIdx.x;\n",
        "    int threadId = threadIdx.x;\n",
        "    int blockSize = blockDim.x;\n",
        "\n",
        "    // Calculate global ID\n",
        "    int globalId = blockId * blockSize + threadId;\n",
        "\n",
        "    // Print the message\n",
        "    printf(\"Hi! My Id is %d, I am the thread %d out of %d in block %d\\n\", globalId, threadId, blockSize, blockId);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define the number of blocks and threads per block\n",
        "    int numBlocks = 5;\n",
        "    int blockSize = 5; // This means each block contains 5 threads\n",
        "\n",
        "    // Launch the kernel\n",
        "    printThreadIds<<<numBlocks, blockSize>>>();\n",
        "\n",
        "    // Wait for GPU to finish before accessing on host\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXgxjlYRhKxT",
        "outputId": "bf029c8a-8761-4455-9632-8d0445d78b5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! My Id is 20, I am the thread 0 out of 5 in block 4\n",
            "Hi! My Id is 21, I am the thread 1 out of 5 in block 4\n",
            "Hi! My Id is 22, I am the thread 2 out of 5 in block 4\n",
            "Hi! My Id is 23, I am the thread 3 out of 5 in block 4\n",
            "Hi! My Id is 24, I am the thread 4 out of 5 in block 4\n",
            "Hi! My Id is 5, I am the thread 0 out of 5 in block 1\n",
            "Hi! My Id is 6, I am the thread 1 out of 5 in block 1\n",
            "Hi! My Id is 7, I am the thread 2 out of 5 in block 1\n",
            "Hi! My Id is 8, I am the thread 3 out of 5 in block 1\n",
            "Hi! My Id is 9, I am the thread 4 out of 5 in block 1\n",
            "Hi! My Id is 15, I am the thread 0 out of 5 in block 3\n",
            "Hi! My Id is 16, I am the thread 1 out of 5 in block 3\n",
            "Hi! My Id is 17, I am the thread 2 out of 5 in block 3\n",
            "Hi! My Id is 18, I am the thread 3 out of 5 in block 3\n",
            "Hi! My Id is 19, I am the thread 4 out of 5 in block 3\n",
            "Hi! My Id is 10, I am the thread 0 out of 5 in block 2\n",
            "Hi! My Id is 11, I am the thread 1 out of 5 in block 2\n",
            "Hi! My Id is 12, I am the thread 2 out of 5 in block 2\n",
            "Hi! My Id is 13, I am the thread 3 out of 5 in block 2\n",
            "Hi! My Id is 14, I am the thread 4 out of 5 in block 2\n",
            "Hi! My Id is 0, I am the thread 0 out of 5 in block 0\n",
            "Hi! My Id is 1, I am the thread 1 out of 5 in block 0\n",
            "Hi! My Id is 2, I am the thread 2 out of 5 in block 0\n",
            "Hi! My Id is 3, I am the thread 3 out of 5 in block 0\n",
            "Hi! My Id is 4, I am the thread 4 out of 5 in block 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory Allocation\n",
        "\n",
        "### 2.Regarding the code:\n",
        "\n",
        "• The code does not work properly. What have you done to\n",
        "correct it?\n",
        "- Add cudaMemcpy calls before kernel execution to transfer input data from host to device memory. Ensure that device memory is freed after the data is copied back to the host to avoid memory leaks.\n",
        "\n",
        "• Change the value of “BLOCKSIZE” to, for instance, “3”.\n",
        "How does it affect the execution compared to the original\n",
        "output?\n",
        " - The kernel will now be launched with fewer threads per block (BLOCKSIZE = 3). This means that in each thread block, only three threads will be active, and since the grid size is 1, only three characters of the string will be modified.\n",
        "\n",
        "- If BLOCKSIZE is less than N, not all elements of a and b will be processed, leading to an incomplete operation. In this case, with BLOCKSIZE = 3, only the first three characters of the string will be modified, and the rest will remain unchanged.\n",
        "In summary, when modifying BLOCKSIZE, it's crucial to ensure that it matches the size of the data being processed to achieve the desired computation across the entire dataset. If there are more data elements than threads, some data will not be processed unless additional thread blocks are added to the grid."
      ],
      "metadata": {
        "id": "ThjZakzehfcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "const int N = 16;\n",
        "const int GRIDSIZE = 1; //number of thread blocks\n",
        "const int BLOCKSIZE = 32; //number of threads per thread block\n",
        "\n",
        "__global__ void hello_decoder(char *a, int *b) {\n",
        "    a[threadIdx.x] += b[threadIdx.x];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    char a[N] = \"Hello\\0\\0\\0\\0\\0\\0\";\n",
        "    int b[N] = {15, 10, 6, 0, -11, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
        "    char *ad;\n",
        "    int *bd;\n",
        "    const int csize = N*sizeof(char);\n",
        "    const int isize = N*sizeof(int);\n",
        "\n",
        "    printf(\"%s \", a);\n",
        "\n",
        "    cudaMalloc((void**)&ad, csize);\n",
        "    cudaMalloc((void**)&bd, isize);\n",
        "\n",
        "    // Copy input data from host to device\n",
        "    cudaMemcpy(ad, a, csize, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(bd, b, isize, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Kernel launch\n",
        "    hello_decoder<<<GRIDSIZE, BLOCKSIZE>>>(ad, bd);\n",
        "\n",
        "    // Copy output data from device to host\n",
        "    cudaMemcpy(a, ad, csize, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(ad);\n",
        "    cudaFree(bd);\n",
        "\n",
        "    printf(\"%s\\n\", a);\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u0xL9SyhfP4",
        "outputId": "0fed3d68-acaa-411f-83f2-fab1a0432926"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\u0001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Provide the kernel code that solves the problem and answer the following questions:\n",
        "\n",
        "__global__ void add(float *x, float *y) {\n",
        " int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "    for (int i = index; i < PROBLEMSIZE; i += stride) {\n",
        "        y[i] += x[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "###• How different is managed transfers between CPU and GPU?\n",
        "\n",
        "Managed memory (cudaMallocManaged) simplifies memory management by providing a single memory space accessible from both CPU and GPU. It allows for automatic data migration between the host and device, eliminating the need for explicit cudaMemcpy calls. However, this can lead to performance overhead due to on-demand paging. In contrast, explicit memory transfers require the programmer to manage separate memory spaces and perform cudaMemcpy operations to move data between host and device.\n",
        "\n",
        "###• Check that it does not return an error (you can attach a screenshot).\n",
        "\n",
        "After the kernel execution, the code checks for errors by verifying that the sum in array y is equal to VALUE. If there is any difference, it prints an error message.\n",
        "\n",
        "###• How long does it take to run (you can use the extension %%time at the beginning of the cell, or the Unix command time before the binary execution)?\n",
        "The actual time taken depend on the GPU's capabilities and the current load on the system. For large problem sizes, such as PROBLEMSIZE = 1000000000, the execution could take a significant amount of time.\n",
        "-Finally the time was:\n",
        "\n",
        "*real\t0m18.612s\n",
        "\n",
        "*user\t0m14.640s\n",
        "\n",
        "*sys\t0m3.229s"
      ],
      "metadata": {
        "id": "pRsfRaVnu7T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile exercise.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "#define VALUE 20\n",
        "#define PROBLEMSIZE 1000000000\n",
        "\n",
        "__global__ void add(float *x, float *y) {\n",
        " int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "    for (int i = index; i < PROBLEMSIZE; i += stride) {\n",
        "        y[i] += x[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    float *x, *y;\n",
        "    cudaMallocManaged(&x, PROBLEMSIZE * sizeof(float));\n",
        "    cudaMallocManaged(&y, PROBLEMSIZE * sizeof(float));\n",
        "    for (int i = 0; i < PROBLEMSIZE; i++) {\n",
        "        float val = (float)(i % VALUE);\n",
        "        x[i] = val;\n",
        "        y[i] = (VALUE - val);\n",
        "    }\n",
        "\n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (PROBLEMSIZE + blockSize - 1) / blockSize;\n",
        "    add<<<numBlocks, blockSize>>>(x, y);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "    float error = 0.0f;\n",
        "    for (int i = 0; i < PROBLEMSIZE; i++)\n",
        "        error = fmax(error, fabs(y[i] - VALUE));\n",
        "    if (error != 0)\n",
        "        printf(\"Wrong result. Check your code, especially your kernel\\n\");\n",
        "\n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9HkyooH_Avl",
        "outputId": "92f6fab2-52c1-4327-d167-67f51807c7dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting exercise.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version\n",
        "! nvcc -o exercise exercise.cu\n",
        "! time ./exercise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RI9-tVUwvfG",
        "outputId": "dbaaa9f9-ba8c-4254-e924-95f93dbfae26"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "\n",
            "real\t0m18.612s\n",
            "user\t0m14.640s\n",
            "sys\t0m3.229s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. If you have not changed the number of blocks and threads, probably you are aware that this code is not leveraging the massively parallel architecture of the GPU.\n",
        "\n",
        "###• Provide block/thread configurations that reduces the execution time. Compare the configurations and the execution time (for instance, with a table).\n",
        "We can optimize the execution of the CUDA kernel by adjusting the block and thread configurations to better utilize the GPU's parallel architecture. The goal is to increase the occupancy of the GPU by launching a sufficient number of blocks and threads so that while some threads are waiting for data to arrive from memory (memory latency).\n",
        "\n",
        "Original Configuration:\n",
        "blockSize = 256: This means each block has 256 threads.\n",
        "numBlocks: It is calculated to cover the entire PROBLEMSIZE, ensuring all elements are processed.\n",
        "\n",
        "Optimization Strategy:\n",
        "Increase the Grid Size: The number of blocks (gridDim.x) should be increased to ensure that there are enough blocks to utilize all the SMs on the GPU. For the given figure, gridDim.x is 1024, which indicates the maximum number of blocks along the x-dimension that can be launched.\n",
        "\n",
        "Modified Kernel Launch:\n",
        "int blockSize = 64;\n",
        "int numBlocks = min(1024, (PROBLEMSIZE + blockSize - 1) / blockSize);\n",
        "//min is used to ensure that we do not launch more blocks than the GPU supports. The 1024 value comes from the GPU's capability as per the figure provided.\n",
        "add<<<numBlocks, blockSize>>>(x, y);\n",
        "\n",
        "###• Is there any limitation on the blocks or threads quantities? Hint: You may check again Figure 5.\n",
        "Thread Configuration: Based on the figure, blockDim.x is 64, suggesting that each block can have up to 64 threads.\n",
        "\n",
        "Calculate Index and Stride: The index calculation doesn't change, but we ensure that it reflects the updated block and thread configuration. The stride is the total number of threads in the grid, which is blockDim.x * gridDim.x.\n",
        "\n",
        "Maximum Threads per Block: Typically, this is 1024 for most modern GPUs.\n",
        "Maximum Blocks per Grid: This is often 2^31 - 1 for each grid dimension but can be lower based on GPU architecture.\n",
        "Maximum Threads per SM: This is hardware-dependent and can be found in the GPU's specification.\n",
        "Shared Memory and Registers: More threads may require more shared memory and registers, which are limited.\n"
      ],
      "metadata": {
        "id": "GbzKk7CL_A5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile optimization.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "#define VALUE 20\n",
        "#define PROBLEMSIZE 1000000000\n",
        "\n",
        "__global__ void add(float *x, float *y) {\n",
        " int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "    for (int i = index; i < PROBLEMSIZE; i += stride) {\n",
        "        y[i] += x[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    float *x, *y;\n",
        "    cudaMallocManaged(&x, PROBLEMSIZE * sizeof(float));\n",
        "    cudaMallocManaged(&y, PROBLEMSIZE * sizeof(float));\n",
        "    for (int i = 0; i < PROBLEMSIZE; i++) {\n",
        "        float val = (float)(i % VALUE);\n",
        "        x[i] = val;\n",
        "        y[i] = (VALUE - val);\n",
        "    }\n",
        "\n",
        "\n",
        "    // start optimization\n",
        "    int blockSize = 64; // Number of threads per block\n",
        "    int numBlocks = min(1024, (PROBLEMSIZE + blockSize - 1) / blockSize); // Number of blocks\n",
        "    add<<<numBlocks, blockSize>>>(x, y);\n",
        "    cudaDeviceSynchronize();\n",
        "    // end optimization\n",
        "\n",
        "\n",
        "\n",
        "    float error = 0.0f;\n",
        "    for (int i = 0; i < PROBLEMSIZE; i++)\n",
        "        error = fmax(error, fabs(y[i] - VALUE));\n",
        "    if (error != 0)\n",
        "        printf(\"Wrong result. Check your code, especially your kernel\\n\");\n",
        "\n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YYTMO9TwvOV",
        "outputId": "35df8787-56f2-444a-9436-622cf87dcb90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing optimization.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version\n",
        "! nvcc -o optimization optimization.cu\n",
        "! time ./optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lamDqR76qUX",
        "outputId": "d268d5ca-a7cf-452d-a1ca-598d9b9a8830"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "\n",
            "real\t0m19.535s\n",
            "user\t0m15.703s\n",
            "sys\t0m3.548s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc  -o optimization optimization.cu\n",
        "! nvprof ./optimization\n",
        "! time ./optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNsHh_Z_LZ60",
        "outputId": "be12eb92-4e0b-44f7-cc0a-d94ac1aa1aeb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==42532== NVPROF is profiling process 42532, command: ./optimization\n",
            "==42532== Profiling application: ./optimization\n",
            "==42532== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  3.29169s         1  3.29169s  3.29169s  3.29169s  add(float*, float*)\n",
            "      API calls:   83.25%  3.29170s         1  3.29170s  3.29170s  3.29170s  cudaDeviceSynchronize\n",
            "                   11.50%  454.77ms         2  227.38ms  188.22ms  266.55ms  cudaFree\n",
            "                    5.24%  207.08ms         2  103.54ms  67.372us  207.01ms  cudaMallocManaged\n",
            "                    0.01%  300.53us         1  300.53us  300.53us  300.53us  cudaLaunchKernel\n",
            "                    0.00%  153.68us       114  1.3480us     145ns  60.834us  cuDeviceGetAttribute\n",
            "                    0.00%  13.976us         1  13.976us  13.976us  13.976us  cuDeviceGetName\n",
            "                    0.00%  5.6440us         1  5.6440us  5.6440us  5.6440us  cuDeviceTotalMem\n",
            "                    0.00%  5.3210us         1  5.3210us  5.3210us  5.3210us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.4340us         3     811ns     248ns  1.5000us  cuDeviceGetCount\n",
            "                    0.00%  1.2940us         2     647ns     269ns  1.0250us  cuDeviceGet\n",
            "                    0.00%     405ns         1     405ns     405ns     405ns  cuModuleGetLoadingMode\n",
            "                    0.00%     264ns         1     264ns     264ns     264ns  cuDeviceGetUuid\n",
            "\n",
            "==42532== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "  191483  40.554KB  4.0000KB  0.9766MB  7.405663GB   1.150202s  Host To Device\n",
            "   22894  170.62KB  4.0000KB  0.9961MB  3.725292GB  339.7858ms  Device To Host\n",
            "   15398         -         -         -           -   3.253582s  Gpu page fault groups\n",
            "Total CPU Page faults: 34341\n",
            "\n",
            "real\t0m19.120s\n",
            "user\t0m14.879s\n",
            "sys\t0m3.341s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile optimization2.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "#define VALUE 20\n",
        "#define PROBLEMSIZE 1000000000\n",
        "\n",
        "__global__ void add(float *x, float *y) {\n",
        " int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "    for (int i = index; i < PROBLEMSIZE; i += stride) {\n",
        "        y[i] += x[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    float *x, *y;\n",
        "    cudaMallocManaged(&x, PROBLEMSIZE * sizeof(float));\n",
        "    cudaMallocManaged(&y, PROBLEMSIZE * sizeof(float));\n",
        "    for (int i = 0; i < PROBLEMSIZE; i++) {\n",
        "        float val = (float)(i % VALUE);\n",
        "        x[i] = val;\n",
        "        y[i] = (VALUE - val);\n",
        "    }\n",
        "\n",
        "\n",
        "    // start optimization\n",
        "    int blockSize = 256; // Number of threads per block\n",
        "    int numBlocks = (PROBLEMSIZE + blockSize - 1) / blockSize; // Number of blocks in the grid\n",
        "    add<<<numBlocks, blockSize>>>(x, y);\n",
        "    cudaDeviceSynchronize();\n",
        "    // end optimization\n",
        "\n",
        "\n",
        "\n",
        "    float error = 0.0f;\n",
        "    for (int i = 0; i < PROBLEMSIZE; i++)\n",
        "        error = fmax(error, fabs(y[i] - VALUE));\n",
        "    if (error != 0)\n",
        "        printf(\"Wrong result. Check your code, especially your kernel\\n\");\n",
        "\n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thzFn26i8WIm",
        "outputId": "7269b076-5ae0-4a06-823e-f9c034af1fe4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting optimization2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Provide the code of the non-managed CPU-GPU memory version of the problem.\n",
        "\n",
        "###• Which are the differences in the dominant API calls when the block size is equal to “1” when comparing managed and non-managed versions? Why?\n",
        "\n",
        "Managed Version: Uses cudaMallocManaged() for memory accessible by both the CPU and GPU, simplifying data transfer.\n",
        "\n",
        "Unmanaged Version: Requires explicit memory allocation and transfer between CPU and GPU using cudaMalloc() and cudaMemcpy().\n",
        "\n",
        "Impact on Performance:\n",
        "Both versions suffer from limited performance with a block size of \"1,\" not leveraging the GPU's parallel capability.\n",
        "\n",
        "Managed Version: May have additional overhead due to automatic memory management.\n",
        "\n",
        "Unmanaged Version: Offers explicit control over memory but is still limited by low parallelism.\n",
        "\n",
        "Why?\n",
        "Low utilization of the GPU's parallel resources affects both versions.\n",
        "Automatic memory management in the managed version does not compensate for the low parallel utilization.\n",
        "\n",
        "The unmanaged version, while more predictable in memory management, also faces limitations of parallelism.\n",
        "\n",
        "In summary, the main difference lies in memory management, but both versions have limited performance due to low use of the GPU's parallelism."
      ],
      "metadata": {
        "id": "0NSTQCsm92jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile non_managed.cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "#define VALUE 20\n",
        "#define PROBLEMSIZE 1000000000\n",
        "\n",
        "__global__ void add(float *x, float *y) {\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < PROBLEMSIZE; i += stride) {\n",
        "    y[i] += x[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "  float *x, *y;\n",
        "  float *d_x, *d_y; // Device pointers\n",
        "  size_t size = static_cast<size_t>(PROBLEMSIZE) * sizeof(float); // Corrected size type\n",
        "\n",
        "  // Allocate host memory\n",
        "  x = (float*)malloc(size);\n",
        "  y = (float*)malloc(size);\n",
        "  if (x == nullptr || y == nullptr) {\n",
        "    std::cerr << \"Failed to allocate host memory!\" << std::endl;\n",
        "    return -1;\n",
        "  }\n",
        "\n",
        "  // Initialize host arrays\n",
        "  for (int i = 0; i < PROBLEMSIZE; i++) {\n",
        "    float val = static_cast<float>(i % VALUE);\n",
        "    x[i] = val;\n",
        "    y[i] = (VALUE - val);\n",
        "  }\n",
        "\n",
        "  // Allocate device memory\n",
        "  cudaMalloc(&d_x, size);\n",
        "  cudaMalloc(&d_y, size);\n",
        "\n",
        "  // Copy data from host to device\n",
        "  cudaMemcpy(d_x, x, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_y, y, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Execute the kernel\n",
        "  int blockSize = 5000;\n",
        "  int numBlocks = (PROBLEMSIZE + blockSize - 1) / blockSize;\n",
        "  add<<<numBlocks, blockSize>>>(d_x, d_y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Copy data back from device to host\n",
        "  cudaMemcpy(y, d_y, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Check for errors (all values should be 20.0f)\n",
        "  float error = 0.0f;\n",
        "  for (int i = 0; i < PROBLEMSIZE; i++)\n",
        "    error = fmax(error, fabs(y[i] - VALUE));\n",
        "  if (error != 0)\n",
        "    printf(\"Wrong result. Check your code, especially your kernel\\n\");\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou9AQIz893iP",
        "outputId": "08c773ae-4543-4987-a07b-61f29d4ec785"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting non_managed.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc -o non_managed non_managed.cu\n",
        "! time ./non_managed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3STVv-iOAJTr",
        "outputId": "f1e336f3-5d28-475d-e487-3da1fc7a1439"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong result. Check your code, especially your kernel\n",
            "\n",
            "real\t0m18.685s\n",
            "user\t0m14.191s\n",
            "sys\t0m4.423s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvprof ./non_managed\n",
        "! time ./non_managed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K70jlOop92GK",
        "outputId": "f8c07fcf-70eb-4d36-d100-4cfddb240462"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==55249== NVPROF is profiling process 55249, command: ./non_managed\n",
            "==55249== Profiling application: ./non_managed\n",
            "==55249== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   63.83%  1.69949s         2  849.74ms  848.32ms  851.17ms  [CUDA memcpy HtoD]\n",
            "                   34.45%  917.31ms         1  917.31ms  917.31ms  917.31ms  [CUDA memcpy DtoH]\n",
            "                    1.72%  45.725ms         1  45.725ms  45.725ms  45.725ms  add(float*, float*)\n",
            "      API calls:   91.41%  2.61766s         3  872.55ms  848.53ms  917.71ms  cudaMemcpy\n",
            "                    6.70%  191.90ms         2  95.951ms  3.4305ms  188.47ms  cudaMalloc\n",
            "                    1.60%  45.735ms         1  45.735ms  45.735ms  45.735ms  cudaDeviceSynchronize\n",
            "                    0.27%  7.8064ms         2  3.9032ms  2.9312ms  4.8752ms  cudaFree\n",
            "                    0.01%  230.26us         1  230.26us  230.26us  230.26us  cudaLaunchKernel\n",
            "                    0.00%  141.14us       114  1.2380us     150ns  55.367us  cuDeviceGetAttribute\n",
            "                    0.00%  11.362us         1  11.362us  11.362us  11.362us  cuDeviceGetName\n",
            "                    0.00%  5.4420us         1  5.4420us  5.4420us  5.4420us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.3400us         1  5.3400us  5.3400us  5.3400us  cuDeviceTotalMem\n",
            "                    0.00%  1.7090us         3     569ns     205ns  1.2440us  cuDeviceGetCount\n",
            "                    0.00%  1.2660us         2     633ns     189ns  1.0770us  cuDeviceGet\n",
            "                    0.00%     484ns         1     484ns     484ns     484ns  cuModuleGetLoadingMode\n",
            "                    0.00%     274ns         1     274ns     274ns     274ns  cuDeviceGetUuid\n",
            "\n",
            "real\t0m18.550s\n",
            "user\t0m14.288s\n",
            "sys\t0m4.144s\n"
          ]
        }
      ]
    }
  ]
}